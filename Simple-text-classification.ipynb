{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Setfit Framework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must first open this notebook in google colab "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MouadEt-tali/setfit/blob/main/Simple-text-classification.ipynb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setfit is based on <font color=\"yellow\">Sentence Transformers </font>  which are modifications of pretrained transformer models that use `Siamese and triplet network structures` to derive **semantically meaningful** sentence embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of these models is to minimize the distance between pairs of semantically similar sentences and maximize the distance between sentence pairs that are semantically distant. Standard STs output a fixed, dense vector that is meant to represent textual data and can then be used by machine learning algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification analyogy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Siamese networks, we take an input image of a person and find out the encodings of that image, then, we take the same network without performing any updates on weights or biases and input an image of a different person and again predict itâ€™s encodings. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/MouadEt-tali/setfit/blob/main/simese_network.png?raw=true\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare these two encodings to check whether there is a similarity between the two images. These two encodings act as a latent feature representation of the images. Images with the same person have similar features/encodings. Using this, we compare and tell if the two images are the same person or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering, how to actually train the network? you can train the network by taking an anchor image and comparing it with both a positive sample and a negative sample. The dissimilarity between the anchor image and positive image must low and the dissimilarity between the anchor image and the negative image must be high.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/MouadEt-tali/setfit/blob/main/triplet_loss.png?raw=true\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setfit Approach to few-shot text classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETFIT uses a two-step training approach in which we first fine-tune a ST  (Sentence Transformer) and then train a classifier head.\n",
    "\n",
    "\n",
    "In the first step, the ST is fine-tuned on the input data in a **contrastive, Siamese manner on sentence pairs.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, a text classification head is trained using the encoded training data generated by the fine-tuned ST from the first step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/MouadEt-tali/setfit/blob/main/setfit_architecture.png?raw=true\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to our analogy Setfit starts by creating pairs of training examples in the following manner:\n",
    "\n",
    "Suppose you have a sentiment analysis task where you have a dataset of sentences with corresponding labels. For simplicity let's assume we have only 2 classes (negative sentiment 0 and positive sentiment 1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That movie was awesome, I wish I could watch it all over again     1 \n",
      "LOOVED IT, next time I'll bring my kids                            1 \n",
      "I was totally DISAPPOINTED, the plot was horrible as well          0 \n"
     ]
    }
   ],
   "source": [
    "example = [\"That movie was awesome, I wish I could watch it all over again \" , \"LOOVED IT, next time I'll bring my kids\" ,\"I was totally DISAPPOINTED, the plot was horrible as well \"]\n",
    "labels  = [1, 1, 0]\n",
    "\n",
    "for i in range(len(example)):\n",
    "    print(f'{example[i]:{65}} {labels[i]:{2}} ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea behind of the sentence pairs generation is that it is possible to **map** the sentences to a feature space where similar sentences are close, and dissimilar sentences are far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/MouadEt-tali/setfit/blob/main/feature_space.png?raw=true\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map our example to this feature space we need to create training example following the siamese/triplet network ideas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('That movie was awesome, I wish I could watch it all over again ',\n",
       "  \"LOOVED IT, next time I'll bring my kids\",\n",
       "  1),\n",
       " ('That movie was awesome, I wish I could watch it all over again ',\n",
       "  'I was totally DISAPPOINTED, the plot was horrible as well ',\n",
       "  0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  (sentence 1 , sentence 2 , label )  where label is 1 if the sentences belong to the same class and 0 if they don't\n",
    "train_examples = [(example[0],example[1],1),(example[0],example[2],0)]\n",
    "train_examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and these train_examples are what we use to fine tune our Sentence Transformers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ST is fine tuned we encode the original sentences simply by calling  \n",
    "\n",
    "`ST.encode(example)`  \n",
    "\n",
    "remember that example contained only the sentences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally now that we have rich vector embeddings for each sentence that contain in them a notion of distance between positive and negative labels, we can train a simple classification model on these vectors. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models, datasets, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_pairs_generation(sentences, labels, pairs):\n",
    "\t# initialize two empty lists to hold the (sentence, sentence) pairs and\n",
    "\t# labels to indicate if a pair is positive or negative\n",
    "\n",
    "  numClassesList = np.unique(labels)\n",
    "  idx = [np.where(labels == i)[0] for i in numClassesList]\n",
    "\n",
    "  for idxA in range(len(sentences)):      \n",
    "    currentSentence = sentences[idxA]\n",
    "    label = labels[idxA]\n",
    "    idxB = np.random.choice(idx[np.where(numClassesList==label)[0][0]])\n",
    "    posSentence = sentences[idxB]\n",
    "\t\t  # prepare a positive pair and update the sentences and labels\n",
    "\t\t  # lists, respectively\n",
    "    pairs.append(InputExample(texts=[currentSentence, posSentence], label=1.0))\n",
    "\n",
    "    negIdx = np.where(labels != label)[0]\n",
    "    negSentence = sentences[np.random.choice(negIdx)]\n",
    "\t\t  # prepare a negative pair of images and update our lists\n",
    "    pairs.append(InputExample(texts=[currentSentence, negSentence], label=0.0))\n",
    "  \n",
    "\t# return a 2-tuple of our image pairs and labels\n",
    "  return (pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SST-2\n",
    "# Load SST-2 dataset into a pandas dataframe.\n",
    "\n",
    "train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "# Load the test dataset into a pandas dataframe.\n",
    "eval_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', delimiter='\\t', header=None)\n",
    "\n",
    "text_col=train_df.columns.values[0] \n",
    "category_col=train_df.columns.values[1]\n",
    "\n",
    "x_eval = eval_df[text_col].values.tolist()\n",
    "y_eval = eval_df[category_col].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SetFit\n",
    "st_model = 'paraphrase-mpnet-base-v2' #@param ['paraphrase-mpnet-base-v2', 'all-mpnet-base-v1', 'all-mpnet-base-v2', 'stsb-mpnet-base-v2', 'all-MiniLM-L12-v2', 'paraphrase-albert-small-v2', 'all-roberta-large-v1']\n",
    "num_training = 32 #@param [\"8\", \"16\", \"32\", \"54\", \"128\", \"256\", \"512\"] {type:\"raw\"}\n",
    "num_itr = 5 #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"10\"] {type:\"raw\"}\n",
    "plot2d_checkbox = True #@param {type: 'boolean'}\n",
    "\n",
    "set_seed(0)\n",
    "# Equal samples per class training\n",
    "train_df_sample = pd.concat([train_df[train_df[1]==0].sample(num_training), train_df[train_df[1]==1].sample(num_training)])\n",
    "x_train = train_df_sample[text_col].values.tolist()\n",
    "y_train = train_df_sample[category_col].values.tolist()\n",
    "\n",
    "train_examples = [] \n",
    "for x in range(num_itr):\n",
    "  train_examples = sentence_pairs_generation(np.array(x_train), np.array(y_train), train_examples)\n",
    "\n",
    "orig_model = SentenceTransformer(st_model)\n",
    "model = SentenceTransformer(st_model)\n",
    "\n",
    "# S-BERT adaptation \n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=10, show_progress_bar=True)\n",
    "\n",
    "# No Fit\n",
    "X_train_noFT = orig_model.encode(x_train)\n",
    "X_eval_noFT = orig_model.encode(x_eval)\n",
    "\n",
    "sgd =  LogisticRegression()\n",
    "sgd.fit(X_train_noFT, y_train)\n",
    "y_pred_eval_sgd = sgd.predict(X_eval_noFT)\n",
    "\n",
    "print('Acc. No Fit', accuracy_score(y_eval, y_pred_eval_sgd))\n",
    "\n",
    "# With Fit (SetFit)\n",
    "X_train = model.encode(x_train)\n",
    "X_eval = model.encode(x_eval)\n",
    "\n",
    "sgd =  LogisticRegression()\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred_eval_sgd = sgd.predict(X_eval)\n",
    "\n",
    "print('Acc. SetFit', accuracy_score(y_eval, y_pred_eval_sgd))\n",
    "\n",
    "#Plot 2-D 2x2 figures\n",
    "if plot2d_checkbox:   \n",
    "\n",
    "  plt.figure(figsize=(20,10))\n",
    "\n",
    "#Plot X_train_noFit\n",
    "  X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train_noFT))\n",
    "  plt.subplot(221)\n",
    "  plt.title('X_train No Fit')\n",
    "\n",
    "  for i, t in enumerate(set(np.array(y_train))):\n",
    "      idx = np.array(y_train) == t\n",
    "      plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "  plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "#Plot X_eval noFit\n",
    "  X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval_noFT))\n",
    "  plt.subplot(223)\n",
    "  plt.title('X_eval No Fit')\n",
    "\n",
    "  for i, t in enumerate(set(np.array(y_eval))):\n",
    "      idx = np.array(y_eval) == t\n",
    "      plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "  plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "\n",
    "#Plot X_train SetFit\n",
    "  X_embedded = TSNE(n_components=2).fit_transform(np.array(X_train))\n",
    "\n",
    "  plt.subplot(222)\n",
    "  plt.title('X_train SetFit')\n",
    "\n",
    "  for i, t in enumerate(set(np.array(y_train))):\n",
    "      idx = np.array(y_train) == t\n",
    "      plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "  plt.legend(bbox_to_anchor=(1, 1));\n",
    "\n",
    "#Plot X_eval SetFit\n",
    "  X_embedded = TSNE(n_components=2).fit_transform(np.array(X_eval))\n",
    "  plt.subplot(224)\n",
    "  plt.title('X_eval SetFit')\n",
    "\n",
    "  for i, t in enumerate(set(np.array(y_eval))):\n",
    "      idx = np.array(y_eval) == t\n",
    "      plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "  plt.legend(bbox_to_anchor=(1, 1));\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
